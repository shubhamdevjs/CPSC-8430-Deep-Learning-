{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c55043f-8734-42e5-bb7b-05b20bf26923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check and set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edadb1b-63ac-409c-aff7-d55509d9d781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    question_count = 0\n",
    "    \n",
    "    # Traverse through the JSON structure\n",
    "    for entry in data['data']:\n",
    "        for paragraph in entry['paragraphs']:\n",
    "            context_text = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question_text = qa['question']\n",
    "                question_count += 1\n",
    "                # Collect each answer associated with the question\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context_text.lower())\n",
    "                    questions.append(question_text.lower())\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return question_count, contexts, questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d4ebea-aaeb-4a36-a1a8-28cc5f9e63c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training and validation datasets\n",
    "train_question_count, train_contexts, train_questions, train_answers = load_data('../spoken_train-v1.1.json')\n",
    "val_question_count, val_contexts, val_questions, val_answers = load_data('../spoken_test-v1.1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c435dfc2-97c3-4d76-a504-dc5467a652eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_answer_end_position(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        # Convert answer text to lowercase for consistency\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        # Calculate and store the end position of each answer\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "# Apply the function to both training and validation answers\n",
    "add_answer_end_position(train_answers, train_contexts)\n",
    "add_answer_end_position(val_answers, val_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f8dc34-7ea5-48b0-b2a7-6315d2a2a0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set model-specific parameters\n",
    "MAX_LENGTH = 512  # Maximum token length for BERT input\n",
    "MODEL_NAME = \"bert-base-uncased\"  # Using the BERT base uncased model\n",
    "DOC_STRIDE = 128  # Stride for handling long contexts\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Check if padding is on the right side (needed for tokenization step later)\n",
    "padding_side = tokenizer.padding_side == \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf4e4cb-4d4c-415a-98f8-9182945ddae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def truncate_contexts(contexts, answers):\n",
    "    truncated_contexts = []\n",
    "    for i, context in enumerate(contexts):\n",
    "        # Check if context length exceeds MAX_LENGTH\n",
    "        if len(context) > MAX_LENGTH:\n",
    "            answer_start = answers[i]['answer_start']\n",
    "            answer_end = answer_start + len(answers[i]['text'])\n",
    "            mid_point = (answer_start + answer_end) // 2\n",
    "            \n",
    "            # Calculate start and end points to center the answer within MAX_LENGTH\n",
    "            start = max(0, min(mid_point - MAX_LENGTH // 2, len(context) - MAX_LENGTH))\n",
    "            end = start + MAX_LENGTH\n",
    "            truncated_contexts.append(context[start:end])\n",
    "            \n",
    "            # Adjust answer's start position in the truncated context\n",
    "            answers[i]['answer_start'] = max(0, answer_start - start)\n",
    "        else:\n",
    "            truncated_contexts.append(context)\n",
    "    \n",
    "    return truncated_contexts\n",
    "\n",
    "# Apply truncation to training contexts\n",
    "train_contexts_truncated = truncate_contexts(train_contexts, train_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48e8ac1-2990-48f6-ad28-0dff5f0970c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and encode the questions and contexts with truncation and padding\n",
    "train_encodings = tokenizer(\n",
    "    train_questions,\n",
    "    train_contexts_truncated,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    stride=DOC_STRIDE,\n",
    "    padding=True,\n",
    "    return_offsets_mapping=True  # To help with aligning answer spans later\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    val_questions,\n",
    "    val_contexts,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    stride=DOC_STRIDE,\n",
    "    padding=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6831a5-f44f-4a26-a4e9-e5a5f7492d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_answer_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        answer = answers[i]['text']\n",
    "        answer_start = answers[i]['answer_start']\n",
    "        answer_end = answers[i]['answer_end']\n",
    "        \n",
    "        # Retrieve the offset mapping for each token in the encoding\n",
    "        offsets = encodings['offset_mapping'][i]\n",
    "        \n",
    "        # Initialize start and end token positions\n",
    "        start_pos = end_pos = None\n",
    "        for j, (offset_start, offset_end) in enumerate(offsets):\n",
    "            if offset_start <= answer_start < offset_end:\n",
    "                start_pos = j\n",
    "            if offset_start < answer_end <= offset_end:\n",
    "                end_pos = j\n",
    "                break\n",
    "\n",
    "        # Append found positions or default to (0, 0) if not found\n",
    "        start_positions.append(start_pos if start_pos is not None else 0)\n",
    "        end_positions.append(end_pos if end_pos is not None else 0)\n",
    "    \n",
    "    return start_positions, end_positions\n",
    "\n",
    "# Find answer positions in the tokenized data for training and validation sets\n",
    "train_start_positions, train_end_positions = find_answer_positions(train_encodings, train_answers)\n",
    "val_start_positions, val_end_positions = find_answer_positions(val_encodings, val_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac8d2b7-67b6-4079-82b8-b9787b8cf694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update tokenized encodings with the start and end positions\n",
    "train_encodings.update({\n",
    "    'start_positions': train_start_positions,\n",
    "    'end_positions': train_end_positions\n",
    "})\n",
    "\n",
    "val_encodings.update({\n",
    "    'start_positions': val_start_positions,\n",
    "    'end_positions': val_end_positions\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e7be3e-4f64-42a5-b72a-d701be025fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns a dictionary of tensor-encoded data at the specified index\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][idx]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create dataset instances for training and validation\n",
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset = QADataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a769ac-bc1a-4000-9e4b-53466d9eba1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataLoader instances for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0d8eba-1aba-4a5c-a7a6-910069486ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model\n",
    "base_bert_model = BertModel.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bbee4c6-08c3-4a5d-8137-ff34219aeafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomQAModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(CustomQAModel, self).__init__()\n",
    "        self.bert = base_model  # Load the BERT model as a backbone\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Define fully connected layers for span prediction\n",
    "        self.fc1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.fc2 = nn.Linear(768 * 2, 2)  # Output layer for start and end logits\n",
    "        \n",
    "        # Sequential layer with dropout, fully connected, and activation\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.dropout,\n",
    "            self.fc1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.fc2\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Forward pass through BERT\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        \n",
    "        # Concatenate the last and third-last hidden states\n",
    "        hidden_states = outputs.hidden_states\n",
    "        combined_hidden = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)\n",
    "        \n",
    "        # Pass through classifier to get logits\n",
    "        logits = self.classifier(combined_hidden)\n",
    "        \n",
    "        # Split logits for start and end predictions\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        # Squeeze to remove the last dimension\n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
    "\n",
    "# Instantiate the custom QA model with the loaded BERT model\n",
    "model = CustomQAModel(base_bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee7e624-da02-4768-97a7-9a2cee4041fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def focal_loss(start_logits, end_logits, start_positions, end_positions, gamma=1.0):\n",
    "    # Softmax for probability calculation\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    probs_start = softmax(start_logits)\n",
    "    probs_end = softmax(end_logits)\n",
    "    \n",
    "    # Inverse probabilities\n",
    "    inv_probs_start = 1 - probs_start\n",
    "    inv_probs_end = 1 - probs_end\n",
    "    \n",
    "    # Log softmax for focal loss computation\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = log_softmax(start_logits)\n",
    "    log_probs_end = log_softmax(end_logits)\n",
    "    \n",
    "    # Negative log-likelihood for focal loss\n",
    "    nll_loss = nn.NLLLoss()\n",
    "    \n",
    "    # Calculate focal loss for start and end positions\n",
    "    focal_start = nll_loss(torch.pow(inv_probs_start, gamma) * log_probs_start, start_positions)\n",
    "    focal_end = nll_loss(torch.pow(inv_probs_end, gamma) * log_probs_end, end_positions)\n",
    "    \n",
    "    # Average the two losses\n",
    "    return (focal_start + focal_end) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b1259e-fee1-494a-bbe3-c784844785e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdamW optimizer with weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "257894af-a246-4fb2-bb0a-6206423402e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_epoch(model, data_loader, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Move batch data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        # Forward pass to get start and end logits\n",
    "        start_logits, end_logits = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Compute the focal loss\n",
    "        loss = focal_loss(start_logits, end_logits, start_positions, end_positions)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track total loss for the epoch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        start_preds = torch.argmax(start_logits, dim=1)\n",
    "        end_preds = torch.argmax(end_logits, dim=1)\n",
    "        batch_accuracy = ((start_preds == start_positions).float().mean() + (end_preds == end_positions).float().mean()) / 2\n",
    "        epoch_accuracy += batch_accuracy.item()\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "    # Return the average loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / batch_count\n",
    "    avg_accuracy = epoch_accuracy / batch_count\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8085bc18-670f-4f6d-81d1-6fe6369480a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batch data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass to get start and end logits\n",
    "            start_logits, end_logits = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            # Calculate focal loss\n",
    "            loss = focal_loss(start_logits, end_logits, start_positions, end_positions)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            start_preds = torch.argmax(start_logits, dim=1)\n",
    "            end_preds = torch.argmax(end_logits, dim=1)\n",
    "            batch_accuracy = ((start_preds == start_positions).float().mean() + (end_preds == end_positions).float().mean()) / 2\n",
    "            eval_accuracy += batch_accuracy.item()\n",
    "\n",
    "            # Decode predictions and references for comparison\n",
    "            for i in range(input_ids.size(0)):\n",
    "                pred_answer = tokenizer.decode(input_ids[i][start_preds[i]:end_preds[i] + 1], skip_special_tokens=True)\n",
    "                true_answer = tokenizer.decode(input_ids[i][start_positions[i]:end_positions[i] + 1], skip_special_tokens=True)\n",
    "                predictions.append(pred_answer)\n",
    "                references.append(true_answer)\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = eval_loss / batch_count\n",
    "    avg_accuracy = eval_accuracy / batch_count\n",
    "    return avg_loss, avg_accuracy, predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b98f907-76cd-4f6c-9ffa-72131b474cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(predictions, references):\n",
    "    # Calculate WER between predictions and references\n",
    "    wer_score = jiwer.wer(references, predictions)\n",
    "    return wer_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c8066f3-287a-4cbb-a6a5-44d6a9670724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training and evaluation...\n",
      "\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.2784, Training Accuracy: 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0385, Validation Accuracy: 0.4520\n",
      "WER for Epoch 1: 1.2941\n",
      "\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9991, Training Accuracy: 0.6753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.3180, Validation Accuracy: 0.4317\n",
      "WER for Epoch 2: 1.1413\n",
      "\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8208, Training Accuracy: 0.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5650, Validation Accuracy: 0.4370\n",
      "WER for Epoch 3: 1.2577\n",
      "\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6713, Training Accuracy: 0.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6840, Validation Accuracy: 0.4469\n",
      "WER for Epoch 4: 1.4817\n",
      "\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5428, Training Accuracy: 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3613, Validation Accuracy: 0.4044\n",
      "WER for Epoch 5: 1.5595\n",
      "\n",
      "Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4169, Training Accuracy: 0.8455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:42<00:00, 71.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3431, Validation Accuracy: 0.4266\n",
      "WER for Epoch 6: 2.1712\n",
      "\n",
      "Final WER Scores: [1.2941071388696603, 1.1413259495790813, 1.2576647388517965, 1.481700645334167, 1.559542683607619, 2.1712256883192285]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "# Move the model to the specified device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# List to store WER scores for each epoch\n",
    "wer_scores = []\n",
    "\n",
    "print(\"Starting training and evaluation...\")\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model_epoch(model, train_loader, optimizer)  # Ensure to pass epoch number\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_loss, val_accuracy, predictions, references = evaluate_model(model, val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Postprocess predictions and references\n",
    "    predictions = [\"$\" if pred == \"\" else pred for pred in predictions]\n",
    "    references = [\"$\" if ref == \"\" else ref for ref in references]\n",
    "\n",
    "    # Calculate WER for the validation predictions\n",
    "    wer_score = calculate_wer(predictions, references)\n",
    "    wer_scores.append(wer_score)\n",
    "    print(f\"WER for Epoch {epoch + 1}: {wer_score:.4f}\")\n",
    "\n",
    "print(\"\\nFinal WER Scores:\", wer_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1f0ea-c23e-4fec-a9c2-b80fa8c7f72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
