{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7f4bb4-0b63-4294-9fd9-455f2a860fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Setting the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25596c3c-03d4-473a-8410-6ecce5ac398b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    total_questions, possible_answers, impossible_answers = 0, 0, 0\n",
    "\n",
    "    for entry in data['data']:\n",
    "        for paragraph in entry['paragraphs']:\n",
    "            text = paragraph['context']\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                q_text = qa_pair['question']\n",
    "                total_questions += 1\n",
    "                for ans in qa_pair['answers']:\n",
    "                    contexts.append(text.lower())\n",
    "                    questions.append(q_text.lower())\n",
    "                    answers.append(ans)\n",
    "                    possible_answers += 1  # Assuming this refers to answer count\n",
    "\n",
    "    return total_questions, possible_answers, impossible_answers, contexts, questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c4cda5-9241-4c4f-a94d-cb5a2fbdc9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "total_train_q, train_pos_ans, train_imp_ans, train_contexts, train_questions, train_answers = load_data(\n",
    "    '../spoken_train-v1.1.json'\n",
    ")\n",
    "\n",
    "# Loading validation data\n",
    "total_val_q, val_pos_ans, val_imp_ans, val_contexts, val_questions, val_answers = load_data(\n",
    "    '../spoken_test-v1.1.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89231895-01c5-44d2-8f4d-01c422a8575f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_answer_end(answers, contexts):\n",
    "    for ans, ctx in zip(answers, contexts):\n",
    "        ans['text'] = ans['text'].lower()\n",
    "        ans['answer_end'] = ans['answer_start'] + len(ans['text'])  # Using 'answer_start' as the starting position key\n",
    "\n",
    "# Adding end positions to answers in training and validation data\n",
    "append_answer_end(train_answers, train_contexts)\n",
    "append_answer_end(val_answers, val_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822cc38d-e83a-4dda-b5d2-f2e06f953070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set maximum sequence length and document stride for handling lengthy contexts\n",
    "MAX_SEQ_LEN = 512\n",
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "doc_stride_value = 128\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL)\n",
    "\n",
    "# Tokenize and encode training data with stride and padding\n",
    "train_encoded = tokenizer(\n",
    "    train_questions,\n",
    "    train_contexts,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    truncation=True,\n",
    "    stride=doc_stride_value,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Tokenize and encode validation data with the same settings\n",
    "val_encoded = tokenizer(\n",
    "    val_questions,\n",
    "    val_contexts,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    truncation=True,\n",
    "    stride=doc_stride_value,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee35b843-038c-419c-94e7-886e0a43d739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_answer_positions_train(index):\n",
    "    start_idx, end_idx = 0, 0\n",
    "    answer_tokens = tokenizer(train_answers[index]['text'], max_length=MAX_SEQ_LEN, truncation=True, padding=True)\n",
    "\n",
    "    for position in range(len(train_encoded['input_ids'][index]) - len(answer_tokens['input_ids'])):\n",
    "        is_match = True\n",
    "        for i in range(1, len(answer_tokens['input_ids']) - 1):\n",
    "            if answer_tokens['input_ids'][i] != train_encoded['input_ids'][index][position + i]:\n",
    "                is_match = False\n",
    "                break\n",
    "        if is_match:\n",
    "            start_idx = position + 1\n",
    "            end_idx = start_idx + len(answer_tokens['input_ids']) - 2\n",
    "            break\n",
    "\n",
    "    return start_idx, end_idx\n",
    "\n",
    "# Ensure positions are on the GPU when used\n",
    "start_positions_train = []\n",
    "end_positions_train = []\n",
    "for idx in range(len(train_encoded['input_ids'])):\n",
    "    start_pos, end_pos = find_answer_positions_train(idx)\n",
    "    start_positions_train.append(start_pos)\n",
    "    end_positions_train.append(end_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16ffc68-70b7-4be2-a275-9f1abca55135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Update the encoded training data with start and end positions\n",
    "train_encoded.update({\n",
    "    'start_positions': torch.tensor(start_positions_train).to(device),\n",
    "    'end_positions': torch.tensor(end_positions_train).to(device)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60d5ca5-8e47-478f-8f2c-649b14a9c732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_answer_positions_valid(index):\n",
    "    start_idx, end_idx = 0, 0\n",
    "    answer_tokens = tokenizer(val_answers[index]['text'], max_length=MAX_SEQ_LEN, truncation=True, padding=True)\n",
    "\n",
    "    for position in range(len(val_encoded['input_ids'][index]) - len(answer_tokens['input_ids'])):\n",
    "        is_match = True\n",
    "        for i in range(1, len(answer_tokens['input_ids']) - 1):\n",
    "            if answer_tokens['input_ids'][i] != val_encoded['input_ids'][index][position + i]:\n",
    "                is_match = False\n",
    "                break\n",
    "        if is_match:\n",
    "            start_idx = position + 1\n",
    "            end_idx = start_idx + len(answer_tokens['input_ids']) - 2\n",
    "            break\n",
    "\n",
    "    return start_idx, end_idx\n",
    "\n",
    "# Generate start and end positions for validation data, ensuring they are on the GPU\n",
    "start_positions_valid = []\n",
    "end_positions_valid = []\n",
    "for idx in range(len(val_encoded['input_ids'])):\n",
    "    start_pos, end_pos = find_answer_positions_valid(idx)\n",
    "    start_positions_valid.append(start_pos)\n",
    "    end_positions_valid.append(end_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f102f7cc-cd87-4673-8447-b8cb567d223d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update the encoded validation data with start and end positions\n",
    "val_encoded.update({\n",
    "    'start_positions': torch.tensor(start_positions_valid).to(device),\n",
    "    'end_positions': torch.tensor(end_positions_valid).to(device)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915be751-8046-4282-90d8-f94c04ac6f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAInputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]).to(device),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][idx]).to(device),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]).to(device),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = QAInputDataset(train_encoded)\n",
    "val_dataset = QAInputDataset(val_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea094a0-c0b1-4b59-9038-9b73b2712456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data loaders for batching the datasets\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e729f4-70b1-432d-b430-ea64ca7b0e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuestionAnsweringModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
    "        self.dropout_layer = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.fc2 = nn.Linear(768 * 2, 2)\n",
    "        self.model_layers = nn.Sequential(\n",
    "            self.dropout_layer,\n",
    "            self.fc1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.fc2\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = bert_output[2]\n",
    "        concatenated_outputs = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)\n",
    "        logits = self.model_layers(concatenated_outputs)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Initialize the model\n",
    "model = QuestionAnsweringModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68030c03-cc01-436d-966b-71444e9bc29b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def focal_loss(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    start_probs = softmax(start_logits)\n",
    "    inv_start_probs = 1 - start_probs\n",
    "    end_probs = softmax(end_logits)\n",
    "    inv_end_probs = 1 - end_probs\n",
    "\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = log_softmax(start_logits)\n",
    "    log_probs_end = log_softmax(end_logits)\n",
    "\n",
    "    nll_loss = nn.NLLLoss()\n",
    "\n",
    "    focal_loss_start = nll_loss(torch.pow(inv_start_probs, gamma) * log_probs_start, start_positions)\n",
    "    focal_loss_end = nll_loss(torch.pow(inv_end_probs, gamma) * log_probs_end, end_positions)\n",
    "\n",
    "    return (focal_loss_start + focal_loss_end) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1475d8b8-0c7d-4111-bf18-ab8c6376b72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess_predictions(start_predictions, end_predictions):\n",
    "    for i in range(len(start_predictions)):\n",
    "        if end_predictions[i] < start_predictions[i]:\n",
    "            end_predictions[i] = start_predictions[i]  # Ensure valid span\n",
    "    return start_predictions, end_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1dc723b-5e7c-4e52-8a8d-84eafd556b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "EPOCHS=6\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "\n",
    "# Set up the linear learning rate scheduler\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea55585-e88c-44c6-b108-b4fe331818b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_epoch(model, dataloader, epoch_number, accumulation_steps=4):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    scaler = GradScaler()  # Initialize the GradScaler for AMP\n",
    "    batch_counter = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(tqdm(dataloader, desc='Training')):  # Updated description\n",
    "        optimizer.zero_grad()  # Reset gradients at the start of each batch\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        with autocast():  # Enable mixed precision\n",
    "            start_output, end_output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            loss = focal_loss(start_output, end_output, start_positions, end_positions, gamma=1)\n",
    "            loss_values.append(loss.item())\n",
    "            scaler.scale(loss).backward()  # Backpropagate the scaled loss\n",
    "\n",
    "        # Perform optimizer step only after accumulating gradients\n",
    "        if (batch_index + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)  # Update weights\n",
    "            scaler.update()  # Update the scaler for the next iteration\n",
    "            optimizer.zero_grad()  # Reset gradients for the next accumulation\n",
    "\n",
    "        start_predictions = torch.argmax(start_output, dim=1)\n",
    "        end_predictions = torch.argmax(end_output, dim=1)\n",
    "\n",
    "        accuracy_values.append(((start_predictions == start_positions).sum() / len(start_predictions)).item())\n",
    "        accuracy_values.append(((end_predictions == end_positions).sum() / len(end_predictions)).item())\n",
    "\n",
    "        batch_counter += 1\n",
    "        if batch_counter == 250 and epoch_number == 1:\n",
    "            avg_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "            print(f'Average Accuracy after {batch_counter} batches: {avg_accuracy}')\n",
    "\n",
    "    # Final step in case there are remaining gradients not updated\n",
    "    if len(dataloader) % accumulation_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()  # Ensure gradients are reset after the last step\n",
    "\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    average_loss = sum(loss_values) / len(loss_values)\n",
    "    return average_accuracy, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd443ec6-ef7a-43a8-bbf0-9c17bfd374e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    answer_pairs = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            start_output, end_output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            start_predictions = torch.argmax(start_output, dim=1)\n",
    "            end_predictions = torch.argmax(end_output, dim=1)\n",
    "\n",
    "            # Postprocess predictions to ensure valid spans\n",
    "            for i in range(len(start_predictions)):\n",
    "                if end_predictions[i] < start_predictions[i]:\n",
    "                    end_predictions[i] = start_predictions[i]  # Ensure valid end index\n",
    "\n",
    "            # Extract answers for each sample in the batch\n",
    "            for i in range(input_ids.size(0)):  # Loop over each sample in the batch\n",
    "                predicted_answer = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.convert_ids_to_tokens(input_ids[i][start_predictions[i]:end_predictions[i]])\n",
    "                )\n",
    "                true_answer = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.convert_ids_to_tokens(input_ids[i][start_true[i]:end_true[i]])\n",
    "                )\n",
    "                answer_pairs.append([predicted_answer, true_answer])\n",
    "\n",
    "    return answer_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88aae195-a9bf-466f-8c6b-bbfe102e8f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "\n",
    "# # Set the multiprocessing start method to 'spawn'\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "from sklearn.metrics import f1_score  # Import the f1_score function\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_start_predictions = []\n",
    "    all_end_predictions = []\n",
    "    all_start_true = []\n",
    "    all_end_true = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            start_output, end_output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            start_predictions = torch.argmax(start_output, dim=1)\n",
    "            end_predictions = torch.argmax(end_output, dim=1)\n",
    "\n",
    "            # Postprocess predictions to ensure valid spans\n",
    "            for i in range(len(start_predictions)):\n",
    "                if end_predictions[i] < start_predictions[i]:\n",
    "                    end_predictions[i] = start_predictions[i]  # Ensure valid end index\n",
    "\n",
    "            # Store predictions and true values for F1 calculation\n",
    "            all_start_predictions.extend(start_predictions.cpu().numpy())\n",
    "            all_end_predictions.extend(end_predictions.cpu().numpy())\n",
    "            all_start_true.extend(start_true.cpu().numpy())\n",
    "            all_end_true.extend(end_true.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_start = f1_score(all_start_true, all_start_predictions, average='weighted')\n",
    "    f1_end = f1_score(all_end_true, all_end_predictions, average='weighted')\n",
    "\n",
    "    print(f\"F1 Score (Start): {f1_start}\")\n",
    "    print(f\"F1 Score (End): {f1_end}\")\n",
    "\n",
    "    return all_start_predictions, all_end_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103c963a-cdbf-4f60-ad0f-fca3ded53ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training:   5%|▌         | 250/4639 [00:25<07:30,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy after 250 batches: 0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5601\n",
      "Training Loss: 1.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.45it/s]\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5320981055960244\n",
      "F1 Score (End): 0.5582406252431552\n",
      "WER Score for Epoch 1: 0.0000\n",
      "Epoch - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6156\n",
      "Training Loss: 1.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.44it/s]\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5438432894585569\n",
      "F1 Score (End): 0.5833630719043977\n",
      "WER Score for Epoch 2: 0.0000\n",
      "Epoch - 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6538\n",
      "Training Loss: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.43it/s]\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5550467734786914\n",
      "F1 Score (End): 0.5896741842488588\n",
      "WER Score for Epoch 3: 0.0000\n",
      "Epoch - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6876\n",
      "Training Loss: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.44it/s]\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5598853609099081\n",
      "F1 Score (End): 0.5954711657107753\n",
      "WER Score for Epoch 4: 0.0000\n",
      "Epoch - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7147\n",
      "Training Loss: 0.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.45it/s]\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Initialize the GradScaler for AMP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5536433336802372\n",
      "F1 Score (End): 0.5871261848539607\n",
      "WER Score for Epoch 5: 0.0000\n",
      "Epoch - 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4639 [00:00<?, ?it/s]/local_scratch/slurm.986220/ipykernel_798407/444267025.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'start_positions': torch.tensor(self.encodings['start_positions'][idx]).to(device),\n",
      "/local_scratch/slurm.986220/ipykernel_798407/444267025.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'end_positions': torch.tensor(self.encodings['end_positions'][idx]).to(device)\n",
      "/local_scratch/slurm.986220/ipykernel_798407/720344242.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Training: 100%|██████████| 4639/4639 [08:00<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7406\n",
      "Training Loss: 0.6763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [03:39<00:00, 72.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Start): 0.5435304243619612\n",
      "F1 Score (End): 0.591615137902125\n",
      "WER Score for Epoch 6: 0.0000\n",
      "WER scores (after adding document stride): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer \n",
    "\n",
    "def calculate_wer(predictions, references):\n",
    "    \"\"\"Calculate the Word Error Rate between predictions and references.\"\"\"\n",
    "    return wer(references, predictions)\n",
    "\n",
    "# Assuming EPOCHS is already defined\n",
    "EPOCHS = 6\n",
    "model.to(device)  # Move model to the specified device\n",
    "wer_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch - {epoch + 1}')\n",
    "    \n",
    "    # Training phase\n",
    "    train_accuracy, train_loss = train_model_epoch(model, train_data_loader, epoch + 1)\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation phase\n",
    "    answer_list = evaluate_model(model, val_data_loader)\n",
    "    predicted_answers = []\n",
    "    true_answers = []\n",
    "\n",
    "    for ans_pair in answer_list:\n",
    "        # Safely check for the predicted and true answers\n",
    "        predicted = ans_pair[0] if isinstance(ans_pair[0], str) and len(ans_pair[0]) > 0 else \"$\"\n",
    "        true = ans_pair[1] if isinstance(ans_pair[1], str) and len(ans_pair[1]) > 0 else \"$\"\n",
    "\n",
    "        # If the answer is an integer, convert it to a string or handle accordingly\n",
    "        if isinstance(ans_pair[0], int):\n",
    "            predicted = str(ans_pair[0])\n",
    "        if isinstance(ans_pair[1], int):\n",
    "            true = str(ans_pair[1])\n",
    "\n",
    "        predicted_answers.append(predicted)\n",
    "        true_answers.append(true)\n",
    "\n",
    "    # Ensure all answers are non-empty\n",
    "    predicted_answers = [ans if ans else \"$\" for ans in predicted_answers]\n",
    "    true_answers = [ans if ans else \"$\" for ans in true_answers]\n",
    "\n",
    "    # Calculate WER score\n",
    "    wer_score = calculate_wer(predicted_answers, true_answers)\n",
    "    print(f\"WER Score for Epoch {epoch + 1}: {wer_score:.4f}\")\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "# Final output of WER scores\n",
    "print('WER scores (after adding document stride):', wer_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa032704-9caf-4288-bd4b-fd322e3a0733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
